# Multiclass Classification

## Multiclass

![](../images/multiclass%20classification%20example.png)

- Multiclass 는 다중 분류 문제를 다룰 때 쓰인다. 0과 1이 아니라.
  - 손으로 쓴 숫자를 판단하는 경우. 0-10 까지.
  - 환자가 여러가지 질병 중 하나를 가진 경우.
- 다중 클래스 분류에서는 각 클래스에 속할 확률을 추정한다.
- Softmax regression 을 쓰면 logistic regression 을 일반화해서 다중 클래스 분류 문제를 해결할 수 있다.

- decision boundary 도 특이해진다. 참고.

## Softmax

- Softmax Regression 은 Logistic Regression 의 일반화 함수로 다중 클래스 분류에 사용할 수 있다.
  - cost function 은 서로 유사하다.

![](../images/softmax%20regression.png)

- Softmax Regression 의 식과 각 확률값은 어떻게 계산되는지에 대한 것.
  - linear function 의 값인 z 가 확률 분포의 값으로 바뀐다.
- a1 - an 을 다 합치면 1이 나옴.

![](../images/softmax%20regression%20cost%20funciton.png)

- softmax regression 에서 loss 는 -log aj 로 계산된다. 손실 함수는 이 aj 를 1에 가깝게 만들도록 하고, y 값으로 예측하도록 만들어준다.

![](../images/softmax%20function%20math%20expression.png)

![](../images/softmax%20cost%20function.png)

## Neural Network with Softmax output

![](../images/neural%20network%20with%20softmax%20output.png)

- Neural Network 에서 다중 클래스 분류 문제를 어떻게 해결하는지.
- Output Layer 에서 Activation function 으로 Softmax function 을 넣으면 된다.
  - 그리고 output layer 의 Unit 의 수도 class 개수만큼 해줘야한다.
- 이를 Tensorflow 로 구현하는건 간단하다. 
  - 똑같이 신경망을 구축하는데 output layer 의 activation function 만 softmax 로 설정하고
  - loss function 을 SparseCategoricalCrossentropy 로 설정하면 된다.
  - 마지막으로 똑같이 모델을 학습시키면 된다.

![](../images/softmax%20tensorflow%20code%20version%201.png)

- 주의할 점은 이 코드보다 더 나은 코드가 있으니 그대로 사용하지 않는 걸 추천한다.

## Improved implementation of softmax

![](../images/improved%20softmax%20in%20tensorflow.png)

- Neural network 에서 Softmax 를 이용하는 경우 개선하는 방법을 다룸.
- 위의 방식은 간단히 구현할 수 있지만 컴퓨터의 소수점 계산 때문에 정확도가 떨어질 수 있다고 한다.
  - logistic regression 이나 softmax 를 이용하는 경우 지수함수로 계산되는데 이 경우 숫자가 아주 크거나 아주 작아질 수 있다. 
  - 컴퓨터에서 오차가 발생할 수 있음.
- 그래서 output layer 에서 linear activation function 을 이용하고 cost function 을 직접 계산하는 방법을 이용한다.
  - TensorFlow 에서 `from_logits=True` 매개변수를 쓰면 된다.
    - 이건 손실 함수에 전달되는 입력 값이 logit 혹은 liner activation function 의 출력 값이라는 걸 말해준다.
    - logit 은 실수 범위의 출력값이고 확률 분포로 변환하기 전의 원시 출력 값이다.
    - loss function 에 softmax operation 이 포함되어야 한다고 알린다.
    - 지수 전인 z 값만으로 softmax 를 돌린다고 생각해보면 이게 numeric round off error 를 해결해줄 수 있곘네.
  - 그리고 이후에 linear activation function 으로 나온 값을 결국 확률로 변환하는 추가 코드가 필요하다.

- Numeric round off error 는 다음과 같다. 

![](../images/numeric%20round%20off%20error.png)

![](../images/improved%20sigmoid%20in%20tensorflow.png)

코드로 보면 이렇다. 
```python
preferred_model = Sequential(
  [
    Dense(25, activation = 'relu'),
    Dense(15, activation = 'relu'),
    Dense(4, activation = 'linear')   #<-- Note
  ]
)
preferred_model.compile(
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note
  optimizer=tf.keras.optimizers.Adam(0.001),
)

preferred_model.fit(
  X_train,y_train,
  epochs=10
)

# Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. 
# Let's look at the preferred model outputs:
p_preferred = preferred_model.predict(X_train)
print(f"two example output vectors:\n {p_preferred[:2]}")
print("largest value", np.max(p_preferred), "smallest value", np.min(p_preferred))

# The output predictions are not probabilities! If the desired output are probabilities, the output should be be processed by a softmax
sm_preferred = tf.nn.softmax(p_preferred).numpy()
print(f"two example output vectors:\n {sm_preferred[:2]}")
print("largest value", np.max(sm_preferred), "smallest value", np.min(sm_preferred))

# To select the most likely category, the softmax is not required. One can find the index of the largest output using [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html).
for i in range(5):
  print( f"{p_preferred[i]}, category: {np.argmax(p_preferred[i])}")
```

## Classification with multiple outputs (Optional)

![](../images/Multi-lable%20classification.png)

- 다중 클래스 분류말고 여기선 다중 레이블 분류를 설명한다.
  - 예를 들면 각 이미지에 여러 레이블이 붙는 경우. 
  - 자율 주행에서 차량 앞의 사진을 보고 차량이 있는지, 버스가 있는지, 보행자가 있는지, 모두 있는지 등의 질문을 하는 것.
  - 이 경우 하나의 이미지에 여러 레이블이 붙는다.
  - 이 경우 목표 Y 는 숫자 벡터의 수로 이뤄진다.

- 다중 레이블 분류를 위한 신경망 구축 방법은 두개다. 
  - 각각 분리해서 신경망을 구축하는 것.
  - 하나의 신경망에서 모두 검출하는 것.
    - 최종 출력 레이어에서 여러개의 뉴런이 되겠지. 위의 에시로는 세 개의 출력 뉴런이고 이게 세 개의 숫자 벡터가 될 것.
