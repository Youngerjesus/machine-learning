# Backpropagation 

## What is a derivative? (Optional)

- 이 글은 Tensorflow 에서 cost function 을 계산하는 방법에 대해서 다루고, 미분, gradient descent, backpropagation 에 대한 기본 개념을 다룬다. 
- backpropagation 으로 신경망에서 cost function 의 미분 값을 계산하고 이와 gradient descent 나 adam 알고리즘을 통해서 파라미터를 업데이트한다. 
  
![](../images/Derivative%20example.png)

![](../images/informal%20definition%20of%20derivative.png)

- 미분이 의미하는 바를 아는게 중요하다.
- 미분의 비공식적인 정의는 미분의 값이 양수이고 이 변수가 증가한다면, 이 원함수도 증가하게 된다는 것이다.
- 미분의 의미를 알면 gradient descent 의 의미도 알 수 있다. 미분 값 * learning rate 만큼 파라미터를 감소시켜서 cost function 을 작게 만드는 것.
  - 미분의 값이 크면 크게 바뀌고, 미분의 값이 작다면 작게 바뀌고. 

- sumpy 를 통해서 파이썬으로 미분 계산을 할 수 있다. 

## Computation graph (Optional)

![](../images/computing%20the%20derivatives.png)

- Computation Graph 를 이용해서 뉴럴 네트워크에서 미분을 계산하는 것을 다룸
- Computation Graph 에서 Backpropagation 을 이용해서 미분을 계산하는 건데 어떻게 계산하는 걸까 
  - 맨 마지막 layer 에서의 미분 계산 값으로부터 타고타고 가면서 계산하면 된다. 
  - cost function 을 줄이려면 d 가 감소하는 방향으로 업데이트 시키고, a 도 감소하는 방향으로, c 도 감소하는 방향으로 이렇이걸
  - 이렇게 뒤에서 계산할 수 밖에 없는 이유가 맨 뒤에 cost function j 가 있기 때문에. 
  - backpropagation 이 효율적인 이유가 뒤에서 부터 계산하니까 이미 한번 계산된 걸 이용하면 되니까.

## Larger neural network example (Optional)

- 큰 신경망에서 Backpropagation 을 적용하는 예제.
