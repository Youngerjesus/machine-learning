# Regression Model 


## Linear Regression 

- ë°ì´í„°ë¥¼ ê°€ì§€ê³  straight line ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒ.
- ex) House size ë¥¼ ê°€ì§€ê³  House price ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ.
- ì‹¤ì œë¡œ ì´ê±¸ ì“¸ ë• ë°ì´í„°ë¥¼ ê°€ì§€ê³  ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ê³ , ì§ì„ ì˜ ìœ í˜•ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸í•´ë³´ê³  ì ìš©í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.

- training set ì„ ê°€ì§€ê³  learning algorithm  ì— ì…ë ¥ì„ í•˜ë©´ function ì„ ë§Œë“ ë‹¤. 
  - ì´ëŸ° function ì„ hypothesis (ê°€ì„¤, ê°€ì •) ì´ë¼ê³  í•œë‹¤. model ì´ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.
  - ì—¬ê¸° ì´ function ì—ë‹¤ê°€ ìƒˆë¡œìš´ input ì„ ë„£ìœ¼ë©´ ì˜ˆì¸¡ì„ í•´ì¤€ë‹¤. ì´ ë•Œì˜ ì˜ˆì¸¡ y ê°’ì„ `y-hat` ì´ë¼ê³ í•œë‹¤. (training set ì˜ y ê°’ì„ target ì´ë¼ê³  í–ˆì—ˆë‹¤. ì´ ì˜ˆì¸¡ê°’ì´ë‘ì€ ì¡°ê¸ˆ ì´ë¦„ì´ ë‹¤ë¦„.)
  - ì´ learning algorithm ì´ Linear regression ì´ ì•„ë‹Œê°€.

- ì ì´ì œ function ì„ ì–´ë–»ê²Œ ë§Œë“¤ ê²ƒì¸ê°€? ìˆ˜í•™ì‹ì„ ì¨ì„œ.
  - function ì€ straight line ì„ ê·¸ë¦°ë‹¤ê³  í•œë‹¤. ì´ê±°ì— ë§ì¶°ì„œ ì‹ì´ ë‚˜ì™€ì•¼í•¨. 
  - $f_[w,b](x) = wx + b$ ì—¬ê¸°ì„œ w ì™€ b ë¥¼ ì •í•´ì£¼ë©´ ì˜ˆì¸¡ê°’ y ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.
  - ë¬¼ë¡  linear function ì´ ì•„ë‹ˆë¼ curve function ì„ ì˜ˆì¸¡í•´ë³¼ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œëŠ” simple í•´ì„œ ì´ê±¸ ì†Œê°œí•¨. (ì‹¤ì œë¡œ í˜„ì‹¤ì˜ ëª¨ë¸ë“¤ì€ linear ê°€ ì•„ë‹Œ ê²½ìš°ê°€ ë§ê² ì§€.)

- one input or feature ë¥¼ ê°€ì§€ê³  linear regression ì„ í•˜ëŠ” ê±¸ univariate (ì¼ë³€ëŸ‰ì˜) linear regression ì´ë¼ê³  í•œë‹¤.
- ë¬¼ë¡  ì´ê±° ë§ê³ ë„ ì—¬ëŸ¬ê°œì˜ Input ì„ ë„£ê³  linear regression ì„ ì ìš©í•´ì„œ function ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

### Terminology 

- Training set: Data used to train the model
- Notation `x`: input or feature or input feature 
- Notation `y`: output or target
- Notation `m`: number of training sets
- Notation `(x, y)`: single training set
- Notation `$(xi, yi)$`: specific `i`th training set

***

## Model Representation lab

Learn to implement the model $f_{w,b}$ for linear regression with one variable

````jupyterpython
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
````

- NumPy, a popular library for scientific computing
- Matplotlib, a popular library for plotting data


### Problem Statement

As in the lecture, you will use the motivating example of housing price prediction.  
This lab will use a simple data set with only two data points - a house with 1000 square feet(sqft) sold for \\$300,000 and a house with 2000 square feet sold for \\$500,000. These two points will constitute our *data or training set*. In this lab, the units of size are 1000 sqft and the units of price are 1000s of dollars.

You would like to fit a linear regression model (shown above as the blue straight line) through these two points, so you can then predict price for other houses - say, a house with 1200 sqft.

### Number of training examples `m`

You will use `m` to denote the number of training examples. Numpy arrays have a `.shape` parameter. `x_train.shape` returns a python tuple with an entry for each dimension. `x_train.shape[0]` is the length of the array and number of examples as shown below.

`````jupyterpython
# m is the number of training examples
print(f"x_train.shape: {x_train.shape}")
m = x_train.shape[0]
print(f"Number of training examples is: {m}")
`````

- x_train.shape: (2,)
- Number of training examples is: 2

One can also use the Python `len()` function as shown below.

````jupyterpython
# m is the number of training examples
m = len(x_train)
print(f"Number of training examples is: {m}")
````

- Number of training examples is: 2

### Training example `x_i, y_i`

You will use (x$^{(i)}$, y$^{(i)}$) to denote the $i^{th}$ training example. Since Python is zero indexed, (x$^{(0)}$, y$^{(0)}$) is (1.0, 300.0) and (x$^{(1)}$, y$^{(1)}$) is (2.0, 500.0).

To access a value in a Numpy array, one indexes the array with the desired offset. For example the syntax to access location zero of `x_train` is `x_train[0]`.
Run the next code block below to get the $i^{th}$ training example.

````jupyterpython
i = 0 # Change this to 1 to see (x^1, y^1)

x_i = x_train[i]
y_i = y_train[i]
print(f"(x^({i}), y^({i})) = ({x_i}, {y_i})")
````

- (x^(0), y^(0)) = (1.0, 300.0)

### Plotting the data

You can plot these two points using the `scatter()` function in the `matplotlib` library, as shown in the cell below.
- The function arguments `marker` and `c` show the points as red crosses (the default is blue dots).

You can use other functions in the `matplotlib` library to set the title and labels to display

````jupyterpython
# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='r')
# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 1000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (1000 sqft)')
plt.show()
````

### Model function

As described in lecture, the model function for linear regression (which is a function that maps from `x` to `y`) is represented as

$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$

The formula above is how you can represent straight lines - different values of $w$ and $b$ give you different straight lines on the plot. <br/> <br/> <br/> <br/> <br/>

Let's try to get a better intuition for this through the code blocks below. Let's start with $w = 100$ and $b = 100$.

**Note: You can come back to this cell to adjust the model's w and b parameters**

````jupyterpython
w = 100
b = 100
print(f"w: {w}")
print(f"b: {b}")
````

- w: 100
- b: 100

Now, let's compute the value of $f_{w,b}(x^{(i)})$ for your two data points. You can explicitly write this out for each data point as -

for $x^{(0)}$, `f_wb = w * x[0] + b`

for $x^{(1)}$, `f_wb = w * x[1] + b`

For a large number of data points, this can get unwieldy and repetitive. So instead, you can calculate the function output in a `for` loop as shown in the `compute_model_output` function below.
> **Note**: The argument description `(ndarray (m,))` describes a Numpy n-dimensional array of shape (m,). `(scalar)` describes an argument without dimensions, just a magnitude.  
> **Note**: `np.zero(n)` will return a one-dimensional numpy array with $n$ entries   

````jupyterpython
def compute_model_output(x, w, b):
    """
    Computes the prediction of a linear model
    Args:
      x (ndarray (m,)): Data, m examples 
      w,b (scalar)    : model parameters  
    Returns
      y (ndarray (m,)): target values
    """
    m = x.shape[0]
    f_wb = np.zeros(m)
    for i in range(m):
        f_wb[i] = w * x[i] + b
        
    return f_wb
````

Now let's call the `compute_model_output` function and plot the output..

````jupyterpython
tmp_f_wb = compute_model_output(x_train, w, b,)

# Plot our model prediction
plt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')

# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')

# Set the title
plt.title("Housing Prices")
# Set the y-axis label
plt.ylabel('Price (in 1000s of dollars)')
# Set the x-axis label
plt.xlabel('Size (1000 sqft)')
plt.legend()
plt.show()
````

### Prediction

Now that we have a model, we can use it to make our original prediction. Let's predict the price of a house with 1200 sqft. Since the units of $x$ are in 1000's of sqft, $x$ is 1.2.

````jupyterpython
w = 200                         
b = 100    
x_i = 1.2
cost_1200sqft = w * x_i + b    

print(f"${cost_1200sqft:.0f} thousand dollars")
````

- $340 thousand dollars

***

## Cost function formula 

- Linear regression ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œ í•´ì•¼í•˜ëŠ” ê²ƒìœ¼ë¡œ cost function ì„ ì •ì˜í•´ì•¼í•œë‹¤.
  - íŠ¸ë ˆì´ë‹ ì…‹ê³¼ ê°€ì¥ ì˜¤ì°¨ê°€ ì ì€ ê°€ì„¤ í•¨ìˆ˜ (hypothesis) ë¥¼ ë„ì¶œí•˜ê¸° ìœ„í•œ ê²ƒ.
- cost function ì€ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤.
- ì•„ê¹Œ ë‚˜ì™”ë˜ f_[w,b](x) = wx + b$ ì—ì„œ w ì™€ b ë¥¼ parameter of model ì´ë¼ê³  í•œë‹¤. ì´ íŒŒë¼ë¯¸í„°ë¥¼ training ì¤‘ì— ì¸¡ì •í•˜ê³  ì´ê²Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢Œìš°í•  ê²ƒì´ë‹¤.
  - ê·¸ë¦¬ê³  w ë¥¼ weight (ê°€ì¤‘ì¹˜) ë¼ê³  ë¶€ë¥´ê³  b ë¥¼ coefficient (ê³„ìˆ˜) ë¼ê³  ë¶€ë¥¸ë‹¤.
- ì–´ë–»ê²Œí•´ì•¼ w ì™€ b ë¥¼ ì˜ ì„¤ê³„í•  ìˆ˜ ìˆì„ê¹Œ? 

- **cost function ì„ ì„¤ê³„í•¨ìœ¼ë¡œì¨ ìš°ë¦¬ ë°ì´í„°ì…‹ì— ë§ëŠ” ìµœì ì˜ í•¨ìˆ˜ë¥¼ ë‚´ë†“ì„ ìˆ˜ ìˆê²Œ ëœë‹¤.**

![](../images/cost%20function.png)

- error = (y-hat - y)
- ì—ëŸ¬ì—ì„œ ì œê³±ì„ í•˜ëŠ” ì´ìœ ëŠ” ê·¸ëƒ¥ ê³„ì‚°í•˜ë©´ 0 ì´ ë‚˜ì˜¤ë‹ˆê¹Œ. ì–‘ìˆ˜ë¡œ ë§Œë“¤ë ¤ê³ .
- m ìœ¼ë¡œ ë‚˜ëˆˆ ì´ìœ ëŠ” training set ì´ ë§ì•„ì§ˆìˆ˜ë¡ cost function ì´ ì»¤ì§ˆ ê²ƒì´ë‹ˆê¹Œ ê·¸ê±° ë§‰ì„ë ¤ê³  
- 2m ìœ¼ë¡œ ë‚˜ëˆˆ ì´ìœ ëŠ” ê³„ì‚°ì„ ê¹”ë”í•˜ê²Œ í•˜ë ¤ê³  ë¯¸ë¶„í•  ë•Œ. 
- ì´ë ‡ê²Œ ë§Œë“  cost function ì„ `the square error cost function` ì´ë¼ê³ í•œë‹¤. 
  - ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì‚¬ëŒë“¤ì€ ê°ê° ë‹¤ë¥¸ cost function ì„ ì‚¬ìš©í•œë‹¤. ê°ê¸° ë‹¤ë¥¸ ìµœì í™”ë¥¼ í•œë‹¤. 
  - linear regression ì—ì„  `the square error cost function` ì´ê±¸ë¡œ ìµœì í™”ë¥¼ í•˜ëŠ” ê±°ê³ .


## Cost function intuition 

- cost function ì„ ì œì¼ ì‘ê²Œ ë§Œë“¤ë„ë¡ í•˜ë©´ model ì˜ íŒŒë¼ë¯¸í„°ì¸ (= ì´ ì˜ˆì‹œì—ì„  w ì™€ b) ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤.
- cost function ì˜ íŒŒë¼ë¯¸í„°ëŠ” w ì™€ b ì¼ê±´ë° ì´ ê·¸ë˜í”„ì˜ ìµœì†Ÿê°’ì´ ê²°êµ­ ì‹¤ì œ ë°ì´í„°ì™€ ì˜ˆì¸¡ì¹˜ ê°’ì˜ ìµœì†Œê°’ì´ë‹ˆê¹Œ ì œì¼ ì •í™•í•œê²Œ ë‚˜ì˜¤ê² ì§€.

![](../images/cost%20function%20visualization.png)

- gradient descent ë¥¼ í†µí•´ì„œ ì´ëŸ° ë³€ìˆ˜ ë‘ ê°œê°€ ìˆëŠ” cost function ì˜ ìµœì†Ÿê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

## Cost function ì‹¤ìŠµ 

### Goal

In this lab you will:
- you will implement and explore the `cost` function for linear regression with one variable. 

`````jupyterpython
import numpy as np
%matplotlib widget
import matplotlib.pyplot as plt
from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick, soup_bowl
plt.style.use('./deeplearning.mplstyle')
`````

- NumPy, a popular library for scientific computing
- Matplotlib, a popular library for plotting data
- local plotting routines in the lab_utils_uni.py file in the local directory

### Problem Statement

You would like a model which can predict housing prices given the size of the house.  
Let's use the same two data points as before the previous lab- a house with 1000 square feet sold for \\$300,000 and a house with 2000 square feet sold for \\$500,000.


| Size (1000 sqft)     | Price (1000s of dollars) |
| -------------------| ------------------------ |
| 1                 | 300                      |
| 2                  | 500                      |

````jupyterpython
x_train = np.array([1.0, 2.0])           #(size in 1000 square feet)
y_train = np.array([300.0, 500.0])           #(price in 1000s of dollars)
````

### Computing Cost
The term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.

The equation for cost with one variable is:
$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{1}$$

where
$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{2}$$

- $f_{w,b}(x^{(i)})$ is our prediction for example $i$ using parameters $w,b$.
- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.
- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(w,b)$.
>Note, in lecture summation ranges are typically from 1 to m, while code will be from 0 to m-1.

The code below calculates cost by looping over each example. In each loop:
- `f_wb`, a prediction is calculated
- the difference between the target and the prediction is calculated and squared.
- this is added to the total cost.

````jupyterpython
def compute_cost(x, y, w, b): 
    """
    Computes the cost function for linear regression.
    
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    
    Returns
        total_cost (float): The cost of using w,b as the parameters for linear regression
               to fit the data points in x and y
    """
    # number of training examples
    m = x.shape[0] 
    
    cost_sum = 0 
    for i in range(m): 
        f_wb = w * x[i] + b   
        cost = (f_wb - y[i]) ** 2  
        cost_sum = cost_sum + cost  
    total_cost = (1 / (2 * m)) * cost_sum  

    return total_cost
````

### Cost Function Intuition 

The cost equation (1) above shows that if $w$ and $b$ can be selected such that the predictions $f_{w,b}(x)$ match the target data $y$, the $(f_{w,b}(x^{(i)}) - y^{(i)})^2 $ term will be zero and the cost minimized. In this simple two point example, you can achieve this!

In the previous lab, you determined that $b=100$ provided an optimal solution so let's set $b$ to 100 and focus on $w$.

<br/>
Below, use the slider control to select the value of $w$ that minimizes cost. It can take a few seconds for the plot to update.


````jupyterpython
plt_intuition(x_train,y_train)
````

The plot contains a few points that are worth mentioning.

- cost is minimized when  ğ‘¤=200 , which matches results from the previous lab
- Because the difference between the target and pediction is squared in the cost equation, the cost increases rapidly when  ğ‘¤ is either too large or too small.
- Using the w and b selected by minimizing cost results in a line which is a perfect fit to the data.

